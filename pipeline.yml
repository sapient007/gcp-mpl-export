resource_types:
  - name: gcs-resource
    type: docker-image
    source:
      repository: frodenas/gcs-resource

resources:
- name: midnight-hour
  type: time
  source:
    start: 1:00 AM
    stop: 2:00 AM
    location: America/New_York

- name: gcs-export-pipelines
  type: git
  source:
    uri: https://github.com/sapient007/gcp-mpl-export.git

- name: rootfs
  type: docker-image
  source:
    repository: pcfnorm/rootfs

- name: gcloud-sdk-image
  type: docker-image
  source:
    repository: google/cloud-sdk

- name: skus
  type: gcs-resource
  source:
    bucket: ((GCS_BUCKET))
    json_key: ((GCP_KEY))
    versioned_file: nd_skus.json

- name: skus-csv
  type: gcs-resource
  source:
    bucket: ((GCS_BUCKET))
    json_key: ((GCP_KEY))
    versioned_file: skus.csv

- name: skus-csv-export
  type: gcs-resource
  source:
    bucket: ((GCS_PUBLIC_BUCKET))
    json_key: ((GCP_PUBLIC_KEY))
    versioned_file: gcp_skus.csv

jobs:
- name: download-gcp-price-json
  plan:
  - aggregate:
    - get: midnight-hour
      trigger: true
    - get: rootfs
    - get: gcs-export-pipelines
  - task: download-gcp-price-json
    image: rootfs
    config:
      platform: linux
      outputs:
      - name: nd_json
      params:
        GCP_JSON_URL: ((SKUS_URL))
      run:
        path: bash
        args:
        - "-c"
        - |
          set -eu
          echo "getting price list from $GCP_JSON_URL"

          cd nd_json
          #fetch json from url 
          curl $GCP_JSON_URL -o skus.json
          #convert to new line deliminated json
          cat skus.json | jq -c '.[]' > nd_skus.json
          #diff nd_skus from previous skus
          echo "skus changed? only show if there's difference"
          diff -B -b -i -q nd_skus.json ../skus/nd_skus.json || true
  - put: skus
    params:
      file: "nd_json/nd_skus.json"

- name: truncate-skus-tables
  plan:
  - aggregate: 
    - get: gcloud-sdk-image
    - get: gcs-export-pipelines
    - get: skus
      passed: [download-gcp-price-json]
      trigger: true
  - task: delete-exisiting-tables
    image: gcloud-sdk-image
    config:
      platform: linux
      inputs:
      - name: gcs-export-pipelines
      params:
        GCP_KEY: ((GCP_KEY))
        GCP_PROJECT: ((GCP_PROJECT))
        BIGQUERY_TABLE: ((BIGQUERY_TABLE))
        BIGQUERY_TABLE_UNNESTED: ((BIGQUERY_TABLE_UNNESTED))
      run:
        path: bash
        args:
        - "-c"
        - |
          set -eu
          #login to gclouod
          echo $GCP_KEY > service.key
          gcloud auth activate-service-account --project=$GCP_PROJECT --key-file=service.key

          #bigquery query
          #drop all rows
          eval "echo \"$(cat gcs-export-pipelines/tasks/delete.rows.sql)\"" | bq query --use_legacy_sql=False --project=$GCP_PROJECT
          eval "echo \"$(cat gcs-export-pipelines/tasks/delete.rows.unnested.sql)\"" | bq query --use_legacy_sql=False --project=$GCP_PROJECT
           #count current rows
          eval "echo \"$(cat gcs-export-pipelines/tasks/count.sql)\"" | bq query --use_legacy_sql=False --project=$GCP_PROJECT
          
- name: import-skus
  plan:
  - aggregate: 
    - get: gcloud-sdk-image
    - get: gcs-export-pipelines
    - get: skus
      passed: [truncate-skus-tables]
      trigger: true
  - task: import-skus
    image: gcloud-sdk-image
    config:
      platform: linux
      inputs:
      - name: gcs-export-pipelines
      - name: skus
      params:
        GCP_KEY: ((GCP_KEY))
        GCP_PROJECT: ((GCP_PROJECT))
        BIGQUERY_TABLE: ((BIGQUERY_TABLE))
        GCS_BUCKET: ((GCS_BUCKET))
        BIGQUERY_TABLE_UNNESTED: ((BIGQUERY_TABLE_UNNESTED))
      run:
        path: bash
        args:
        - "-c"
        - |
          set -eu
          #login to gclouod
          echo $GCP_KEY > service.key
          gcloud auth activate-service-account --project=$GCP_PROJECT --key-file=service.key

          #bigquery query
          bq load \
          --source_format=NEWLINE_DELIMITED_JSON \
          --autodetect \
          --replace=true \
          --max_bad_records=100 \
          $BIGQUERY_TABLE \
          gs://$GCS_BUCKET/nd_skus.json 

          # #unnest table for export  
          cat gcs-export-pipelines/tasks/unnest.skus.sql | bq query --destination_table $BIGQUERY_TABLE_UNNESTED --use_legacy_sql=false 

          #count current rows
          eval "echo \"$(cat gcs-export-pipelines/tasks/count.sql)\"" | bq query --use_legacy_sql=False --project=$GCP_PROJECT


- name: export-skus-csv
  plan:
  - aggregate: 
    - get: gcloud-sdk-image
    - get: gcs-export-pipelines
    - get: skus
      passed: [import-skus]
      trigger: true
  - task: export-skus-cs
    image: gcloud-sdk-image
    config:
      platform: linux
      inputs:
      - name: gcs-export-pipelines
      - name: skus
      params:
        GCP_KEY: ((GCP_KEY))
        GCP_PROJECT: ((GCP_PROJECT))
        BIGQUERY_TABLE: ((BIGQUERY_TABLE))
        BIGQUERY_TABLE_UNNESTED: ((BIGQUERY_TABLE_UNNESTED))
        GCS_BUCKET: ((GCS_BUCKET))
      run:
        path: bash
        args:
        - "-c"
        - |
          set -eu
          #login to gclouod
          echo $GCP_KEY > service.key
          gcloud auth activate-service-account --project=$GCP_PROJECT --key-file=service.key

          #bigquery query
          bq extract \
          --destination_format CSV \
          $BIGQUERY_TABLE_UNNESTED  \
          gs://$GCS_BUCKET/skus.csv

- name: export-gcp-price-csv-public
  plan:
  - aggregate:
    - get: skus
      passed: [export-skus-csv]
      trigger: true    
    - get: skus-csv
    - get: rootfs
  - task: download-gcp-price-json
    image: rootfs
    config:
      platform: linux
      inputs:
      - name: skus-csv
      params:
        GCP_KEY: ((GCP_PUBLIC_KEY))
        GCS_BUCKET: ((GCS_PUBLIC_BUCKET))
      run:
        path: bash
        args:
        - "-c"
        - |
          set -eu
          echo $GCP_KEY > service.key
          gcloud auth activate-service-account --project="dev-project-218414" --key-file=service.key

          echo "transfer container"                   
  - put: skus-csv-export
    params:
      file: "skus-csv/skus.csv"          